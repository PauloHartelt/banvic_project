# Projeto ETL Banvic

## ðŸ“Œ DescriÃ§Ã£o

Este projeto implementa um pipeline de **ETL (Extract, Transform, Load)** utilizando **Apache Airflow** em sua versÃ£o 2.8.1 como orquestrador, **PostgreSQL** como banco de dados fonte e Data Warehouse (DW), e **Docker Compose** para gerenciar os serviÃ§os.

O pipeline realiza:

- **ExtraÃ§Ã£o** de dados:
  - De um arquivo CSV (`transacoes.csv`);
  - De um banco PostgreSQL (dump `banvic.sql`).
- **TransformaÃ§Ã£o** (apenas padronizaÃ§Ã£o e organizaÃ§Ã£o).
  - Armazenar os dados em um caminho padronizado com ano-mÃªs-dia/fonte dos dados/nome da tabela.csv
  - ExecuÃ§Ã£o automÃ¡tica, todos os dias, Ã s 04:35 da manhÃ£.
- **Carga** dos dados extraÃ­dos em um **Data Warehouse PostgreSQL**.

ServiÃ§os docker utilizados:

- **DB**: Banco de dados Postgre fonte, carregado com o banvic.sql.
- **DW**: Banco de dados Postgre destino, produto do projeto.
- **Airflow DB**: Banco de dados interno do Airflow.
- **Airflow init**: ServiÃ§o de inicializaÃ§Ã£o do Airflow.
- **Airflow webserver**: Interface web do Airflow, rodando na porta 8080.
- **Airflow scheduler**: Agendador de tarefas do Airflow.

Como executar a aplicaÃ§Ã£o:

- Execute o comando de instalaÃ§Ã£o das **bibliotecas Python** em seu terminal: pip install -r requirements.txt
- Inicialize o **Docker Desktop** em sua mÃ¡quina.
- Com o docker inicializado, execute o comando para inicializar o **Docker Compose** em sua mÃ¡quina: docker compose up
- Com o docker compose executando, aguarde atÃ© executar o **Airflow**.
- Ao airflow executar, abra o seu **navegador web** e cole o seguinte caminho: http://localhost:8080/login/
- Ao entrar na tela de usuÃ¡rio e senha do **Airflow**, coloque os seguintes dados para ter acesso Ã  ferramenta:
  - Username: admin
  - Password: admin
- Com isso tudo feito, basta clicar no nome da **DAG** e clicar em Trigger DAG, no canto superior direito da tela.

---

## ðŸ—‚ Estrutura do Projeto

```bash
banvic-project/
â”œâ”€â”€ airflow/                  # ConfiguraÃ§Ãµes do Airflow
â”‚   â”œâ”€â”€ dags/                 # DAGs do Airflow
â”‚   â”‚   â””â”€â”€ banvic_dag.py     # DAG principal do ETL
â”‚   â”œâ”€â”€ logs/                 # Logs do Airflow
â”‚   â”œâ”€â”€ plugins/              # Plugins (se necessÃ¡rio)
â”‚   â””â”€â”€ scripts/              # FunÃ§Ãµes auxiliares
â”‚       â””â”€â”€ extract_helpers.py
â”œâ”€â”€ airflow-db-data           # Volume do banco do Airflow
â”œâ”€â”€ banvic.sql                # Script SQL com dados do banco fonte
â”œâ”€â”€ data_sources/             # Fonte dos dados CSV e SQL
â”‚   â”œâ”€â”€ transacoes.csv
â”‚   â””â”€â”€ banvic.sql
â”œâ”€â”€ dbdata/                   # Volume do banco fonte
â”œâ”€â”€ dwdata/                   # Volume do Data Warehouse
â”œâ”€â”€ outputs/                  # SaÃ­da das extraÃ§Ãµes (arquivos .csv)
â”œâ”€â”€ volumes/                  # Pasta de configuraÃ§Ã£o dos logs do Airflow
â”œâ”€â”€ docker-compose.yml        # DefiniÃ§Ã£o dos serviÃ§os (db, dw, airflow, etc.)
â””â”€â”€ README.md                 # Esta documentaÃ§Ã£o
â”œâ”€â”€ requirements.txt          # Bibliotecas Python utilizadas
```
